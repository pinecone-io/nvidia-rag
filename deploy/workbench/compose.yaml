services:

  nim-llm:
    container_name: nim-llm-ms
    image: nvcr.io/nim/nvidia/llama-3.3-nemotron-super-49b-v1:1.8.5
    volumes:
    - ${MODEL_DIRECTORY:-/tmp}:/opt/nim/.cache
    user: "${USERID}"
    ports:
    - "8999:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NGC_API_KEY}
    shm_size: 20gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${LLM_MS_GPU_ID:-1}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/v1/health/ready')"]
      interval: 10s
      timeout: 20s
      retries: 100
    profiles: ["local"]

  nemoretriever-embedding-ms:
    container_name: nemoretriever-embedding-ms
    image: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2:1.9.0
    volumes:
    - ${MODEL_DIRECTORY:-/tmp}:/opt/nim/.cache
    ports:
    - "9080:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NGC_API_KEY}
      NIM_TRT_ENGINE_HOST_CODE_ALLOWED: 1
    user: "${USERID}"
    shm_size: 16GB
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${EMBEDDING_MS_GPU_ID:-0}']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health/ready"]
      interval: 30s
      timeout: 20s
      retries: 3
      start_period: 10m
    profiles: ["local"]

  nemoretriever-ranking-ms:
    container_name: nemoretriever-ranking-ms
    image: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2:1.7.0
    volumes:
    - ${MODEL_DIRECTORY:-/tmp}:/opt/nim/.cache
    ports:
    - "1976:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NGC_API_KEY}
    user: "${USERID}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health/ready"]
      interval: 10s
      timeout: 20s
      retries: 100
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${RANKING_MS_GPU_ID:-0}']
              capabilities: [gpu]
    profiles: ["local"]

  page-elements:
    image: ${YOLOX_IMAGE:-nvcr.io/nim/nvidia/nemoretriever-page-elements-v2}:${YOLOX_TAG:-1.4.0}
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NVIDIA_API_KEY=${NGC_API_KEY:-nvidiaapikey}
      - NGC_API_KEY=${NGC_API_KEY:-nvidiaapikey}
      - CUDA_VISIBLE_DEVICES=0
      - NIM_TRITON_MODEL_BATCH_SIZE=${PAGE_ELEMENTS_BATCH_SIZE:-1}
      # NIM OpenTelemetry Settings
      - NIM_OTEL_SERVICE_NAME=page-elements
      - NIM_OTEL_TRACES_EXPORTER=otlp
      - NIM_OTEL_METRICS_EXPORTER=console
      - NIM_OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
      - NIM_ENABLE_OTEL=true
      # Triton OpenTelemetry Settings
      - TRITON_OTEL_URL=http://otel-collector:4318/v1/traces
      - TRITON_OTEL_RATE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${YOLOX_MS_GPU_ID:-0}']
              capabilities: [gpu]
    runtime: nvidia
    profiles: ["local"]

  graphic-elements:
    image: ${YOLOX_GRAPHIC_ELEMENTS_IMAGE:-nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1}:${YOLOX_GRAPHIC_ELEMENTS_TAG:-1.4.0}
    ports:
      - "8003:8000"
      - "8004:8001"
      - "8005:8002"
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NVIDIA_API_KEY=${NGC_API_KEY:-nvidiaapikey}
      - NGC_API_KEY=${NGC_API_KEY:-nvidiaapikey}
      - CUDA_VISIBLE_DEVICES=0
      - NIM_TRITON_MODEL_BATCH_SIZE=${GRAPHIC_ELEMENTS_BATCH_SIZE:-1}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['${YOLOX_GRAPHICS_MS_GPU_ID:-0}']
              capabilities: [gpu]
    runtime: nvidia
    profiles: ["local"]

  table-structure:
    image: ${YOLOX_TABLE_STRUCTURE_IMAGE:-nvcr.io/nim/nvidia/nemoretriever-table-structure-v1}:${YOLOX_TABLE_STRUCTURE_TAG:-1.4.0}
    ports:
      - "8006:8000"
      - "8007:8001"
      - "8008:8002"
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NGC_API_KEY=${NGC_API_KEY:-nvidiaapikey}
      - CUDA_VISIBLE_DEVICES=0
      - NIM_TRITON_MODEL_BATCH_SIZE=${TABLE_STRUCTURE_BATCH_SIZE:-1}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids:  ['${YOLOX_TABLE_MS_GPU_ID:-0}']
              capabilities: [gpu]
    runtime: nvidia
    profiles: ["local"]

  paddle:
    image: ${PADDLE_IMAGE:-nvcr.io/nim/baidu/paddleocr}:${PADDLE_TAG:-1.4.0}
    shm_size: 2gb
    ports:
      - "8009:8000"
      - "8010:8001"
      - "8011:8002"
    user: root
    environment:
      - NIM_HTTP_API_PORT=8000
      - NIM_TRITON_LOG_VERBOSE=1
      - NVIDIA_API_KEY=${NGC_API_KEY:-nvidiaapikey}
      - NGC_API_KEY=${NGC_API_KEY:-nvidiaapikey}
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids:  ['${OCR_MS_GPU_ID:-${PADDLE_MS_GPU_ID:-0}}']
              capabilities: [gpu]
    runtime: nvidia
    profiles: ["local"]

  # Main ingestor server which is responsible for ingestion
  ingestor-server:
    container_name: ingestor-server
    image: nvcr.io/nvstaging/blueprint/ingestor-server:${TAG:-latest}
    build:
      # Set context to repo's root directory
      context: ../../
      dockerfile: ./src/nvidia_rag/ingestor_server/Dockerfile
    # start the server on port 8082 with 4 workers for improved latency on concurrent requests.
    command: --port 8082 --host 0.0.0.0 --workers 1

    volumes:
      # Mount the prompt.yaml file to the container, path should be absolute
      - ../../src/nvidia_rag/rag_server/prompt.yaml:${PROMPT_CONFIG_FILE:-/prompt.yaml}

    # Common customizations to the pipeline can be controlled using env variables
    environment:
      # Path to example directory relative to root
      EXAMPLE_PATH: 'src/nvidia_rag/ingestor_server'

      # Absolute path to custom prompt.yaml file
      PROMPT_CONFIG_FILE: ${PROMPT_CONFIG_FILE:-/prompt.yaml}

      ##===Vector DB specific configurations===
      # URL on which vectorstore is hosted
      APP_VECTORSTORE_URL: "http://milvus:19530"
      # Type of vectordb used to store embedding supported type milvus
      APP_VECTORSTORE_NAME: "milvus"
      # Type of vectordb search to be used
      APP_VECTORSTORE_SEARCHTYPE: ${APP_VECTORSTORE_SEARCHTYPE:-"dense"} # Can be dense or hybrid
      # Boolean to enable GPU index for milvus vectorstore specific to nvingest
      APP_VECTORSTORE_ENABLEGPUINDEX: ${APP_VECTORSTORE_ENABLEGPUINDEX:-True}
      # Boolean to control GPU search for milvus vectorstore specific to nvingest
      APP_VECTORSTORE_ENABLEGPUSEARCH: ${APP_VECTORSTORE_ENABLEGPUSEARCH:-True}
      # vectorstore collection name to store embeddings
      COLLECTION_NAME: ${COLLECTION_NAME:-multimodal_data}

      ##===MINIO specific configurations===
      MINIO_ENDPOINT: "minio:9010"
      MINIO_ACCESSKEY: "minioadmin"
      MINIO_SECRETKEY: "minioadmin"

      NGC_API_KEY: ${NGC_API_KEY:?"NGC_API_KEY is required"}
      NVIDIA_API_KEY: ${NGC_API_KEY:?"NGC_API_KEY is required"}

      ##===Embedding Model specific configurations===
      # url on which embedding model is hosted. If "", Nvidia hosted API is used
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL-"nemoretriever-embedding-ms:8000"}
      APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}
      APP_EMBEDDINGS_DIMENSIONS: ${APP_EMBEDDINGS_DIMENSIONS:-2048}

      ##===NV-Ingest Connection Configurations=======
      APP_NVINGEST_MESSAGECLIENTHOSTNAME: ${APP_NVINGEST_MESSAGECLIENTHOSTNAME:-"nv-ingest-ms-runtime"}
      APP_NVINGEST_MESSAGECLIENTPORT: ${APP_NVINGEST_MESSAGECLIENTPORT:-7670}

      ##===NV-Ingest Extract Configurations==========
      APP_NVINGEST_EXTRACTTEXT: ${APP_NVINGEST_EXTRACTTEXT:-True}
      APP_NVINGEST_EXTRACTINFOGRAPHICS: ${APP_NVINGEST_EXTRACTINFOGRAPHICS:-False}
      APP_NVINGEST_EXTRACTTABLES: ${APP_NVINGEST_EXTRACTTABLES:-True}
      APP_NVINGEST_EXTRACTCHARTS: ${APP_NVINGEST_EXTRACTCHARTS:-True}
      APP_NVINGEST_EXTRACTIMAGES: ${APP_NVINGEST_EXTRACTIMAGES:-False}
      APP_NVINGEST_PDFEXTRACTMETHOD: ${APP_NVINGEST_PDFEXTRACTMETHOD:-None} # Select from pdfium, nemoretriever_parse, None
      # Extract text by "page" only recommended for documents with pages like .pdf, .docx, etc.
      APP_NVINGEST_TEXTDEPTH: ${APP_NVINGEST_TEXTDEPTH:-page} # extract by "page" or "document"

      ##===NV-Ingest Splitting Configurations========
      APP_NVINGEST_CHUNKSIZE: ${APP_NVINGEST_CHUNKSIZE:-512}
      APP_NVINGEST_CHUNKOVERLAP: ${APP_NVINGEST_CHUNKOVERLAP:-150}
      APP_NVINGEST_ENABLEPDFSPLITTER: ${APP_NVINGEST_ENABLEPDFSPLITTER:-True}
      APP_NVINGEST_SEGMENTAUDIO: ${APP_NVINGEST_SEGMENTAUDIO:-False} # Enable audio segmentation for NV Ingest

      ##===NV-Ingest Caption Model configurations====
      APP_NVINGEST_CAPTIONMODELNAME: ${APP_NVINGEST_CAPTIONMODELNAME:-"nvidia/llama-3.1-nemotron-nano-vl-8b-v1"}
      # Incase of nvidia-hosted caption model, use the endpoint url as - https://integrate.api.nvidia.com/v1
      APP_NVINGEST_CAPTIONENDPOINTURL: ${APP_NVINGEST_CAPTIONENDPOINTURL:-"http://vlm-ms:8000/v1/chat/completions"}

      # Choose whether to store the extracted content in the vector store for citation support
      ENABLE_CITATIONS: ${ENABLE_CITATIONS:-True}

      # Choose the summary model to use for document summary
      SUMMARY_LLM: ${SUMMARY_LLM:-nvidia/llama-3.3-nemotron-super-49b-v1}
      SUMMARY_LLM_SERVERURL: ${SUMMARY_LLM_SERVERURL-"nim-llm:8000"}
      SUMMARY_LLM_MAX_CHUNK_LENGTH: ${SUMMARY_LLM_MAX_CHUNK_LENGTH:-50000}
      SUMMARY_CHUNK_OVERLAP: ${SUMMARY_CHUNK_OVERLAP:-200}
      # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
      LOGLEVEL: ${LOGLEVEL:-INFO}

      # [Optional] Redis configuration for task status and result storage
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
      REDIS_DB: ${REDIS_DB:-0}

      # Bulk upload to MinIO
      ENABLE_MINIO_BULK_UPLOAD: ${ENABLE_MINIO_BULK_UPLOAD:-True}
      TEMP_DIR: ${TEMP_DIR:-/tmp-data}

      # NV-Ingest Batch Mode Configurations
      NV_INGEST_FILES_PER_BATCH: ${NV_INGEST_FILES_PER_BATCH:-16}
      NV_INGEST_CONCURRENT_BATCHES: ${NV_INGEST_CONCURRENT_BATCHES:-4}

    ports:
      - "8082:8082"
    expose:
      - "8082"
    shm_size: 5gb
    profiles: ["ingest"]

  redis:
    image: "redis/redis-stack"
    ports:
      - "6379:6379"
    profiles: ["ingest"]

  nv-ingest-ms-runtime:
    image: nvcr.io/nvidia/nemo-microservices/nv-ingest:25.6.2
    cpuset: "0-15"
    volumes:
      - ${DATASET_ROOT:-./data}:/workspace/data
    ports:
      # HTTP API
      - "7670:7670"
      # Simple Broker
      - "7671:7671"
    cap_add:
      - sys_nice
    environment:
      # Audio model not used in this RAG version
      - AUDIO_GRPC_ENDPOINT=audio:50051
      - AUDIO_INFER_PROTOCOL=grpc
      - CUDA_VISIBLE_DEVICES=0
      - MAX_INGEST_PROCESS_WORKERS=${MAX_INGEST_PROCESS_WORKERS:-16}
      - EMBEDDING_NIM_MODEL_NAME=${EMBEDDING_NIM_MODEL_NAME:-${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}}
      # Incase of self-hosted embedding model, use the endpoint url as - https://integrate.api.nvidia.com/v1
      - EMBEDDING_NIM_ENDPOINT=${EMBEDDING_NIM_ENDPOINT:-${APP_EMBEDDINGS_SERVERURL-http://nemoretriever-embedding-ms:8000/v1}}
      - INGEST_LOG_LEVEL=DEFAULT
      - INGEST_EDGE_BUFFER_SIZE=64
      - INGEST_DYNAMIC_MEMORY_THRESHOLD=0.8
      - INGEST_DISABLE_DYNAMIC_SCALING=${INGEST_DISABLE_DYNAMIC_SCALING:-True}
      - INSTALL_AUDIO_EXTRACTION_DEPS=true
      # Message client for development
      #- MESSAGE_CLIENT_HOST=0.0.0.0
      #- MESSAGE_CLIENT_PORT=7671
      #- MESSAGE_CLIENT_TYPE=simple # Configure the ingest service to use the simple broker
      # Message client for production
      - MESSAGE_CLIENT_HOST=redis
      - MESSAGE_CLIENT_PORT=6379
      - MESSAGE_CLIENT_TYPE=redis
      - MINIO_BUCKET=${MINIO_BUCKET:-nv-ingest}
      - MRC_IGNORE_NUMA_CHECK=1
      - NEMORETRIEVER_PARSE_HTTP_ENDPOINT=${NEMORETRIEVER_PARSE_HTTP_ENDPOINT:-http://nemoretriever-parse:8000/v1/chat/completions}
      - NEMORETRIEVER_PARSE_INFER_PROTOCOL=${NEMORETRIEVER_PARSE_INFER_PROTOCOL:-http}
      - NEMORETRIEVER_PARSE_MODEL_NAME=${NEMORETRIEVER_PARSE_MODEL_NAME:-nvidia/nemoretriever-parse}
      - NVIDIA_API_KEY=${NVIDIA_API_KEY:-nvidiaapikey}
      - NGC_API_KEY=${NGC_API_KEY:-nvidiaapikey}
      - NVIDIA_BUILD_API_KEY=${NGC_API_KEY:-nvidiaapikey}
      - NV_INGEST_MAX_UTIL=${NV_INGEST_MAX_UTIL:-48}
      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317
      # Self-hosted paddle endpoints.
      - OCR_GRPC_ENDPOINT=${OCR_GRPC_ENDPOINT:-${PADDLE_GRPC_ENDPOINT:-paddle:8001}}
      - OCR_HTTP_ENDPOINT=${OCR_HTTP_ENDPOINT:-${PADDLE_HTTP_ENDPOINT:-http://paddle:8000/v1/infer}}
      - OCR_INFER_PROTOCOL=${OCR_INFER_PROTOCOL:-${PADDLE_INFER_PROTOCOL:-grpc}}
      - OCR_MODEL_NAME=${OCR_MODEL_NAME:-paddle}
      # build.nvidia.com hosted paddle endpoints.
      #- OCR_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/baidu/paddleocr
      #- OCR_INFER_PROTOCOL=http
      - READY_CHECK_ALL_COMPONENTS=False
      - REDIS_MORPHEUS_TASK_QUEUE=morpheus_task_queue
      # Self-hosted redis endpoints.
      # build.nvidia.com hosted yolox endpoints.
      #- YOLOX_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2
      #- YOLOX_INFER_PROTOCOL=http
      - YOLOX_GRPC_ENDPOINT=${YOLOX_GRPC_ENDPOINT:-page-elements:8001}
      - YOLOX_HTTP_ENDPOINT=${YOLOX_HTTP_ENDPOINT:-http://page-elements:8000/v1/infer}
      - YOLOX_INFER_PROTOCOL=${YOLOX_INFER_PROTOCOL:-grpc}
      # build.nvidia.com hosted yolox-graphics-elements endpoints.
      #- YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1
      #- YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL=http
      - YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT=${YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT:-graphic-elements:8001}
      - YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT=${YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT:-http://graphic-elements:8000/v1/infer}
      - YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL=${YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL:-grpc}
      # build.nvidia.com hosted  yolox-table-elements endpoints.
      #- YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT=https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1
      #- YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL=http
      - YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT=${YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT:-table-structure:8001}
      - YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT=${YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT:-http://table-structure:8000/v1/infer}
      - YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL=${YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL:-grpc}
      # Incase of nvidia-hosted caption model, use the endpoint url as - https://integrate.api.nvidia.com/v1/chat/completions
      - VLM_CAPTION_ENDPOINT=${VLM_CAPTION_ENDPOINT:-http://vlm-ms:8000/v1/chat/completions}
      - VLM_CAPTION_MODEL_NAME=${VLM_CAPTION_MODEL_NAME:-nvidia/llama-3.1-nemotron-nano-vl-8b-v1}
      - MODEL_PREDOWNLOAD_PATH=${MODEL_PREDOWNLOAD_PATH:-/workspace/models/}
    healthcheck:
      test: curl --fail http://nv-ingest-ms-runtime:7670/v1/health/ready || exit 1
      interval: 10s
      timeout: 5s
      retries: 20
    profiles: ["ingest"]

  # Main orchestrator server which stiches together all calls to different services to fulfill the user request
  rag-server:
    container_name: rag-server
    image: nvcr.io/nvstaging/blueprint/rag-server:${TAG:-latest}
    build:
      # Set context to repo's root directory
      context: ../../
      dockerfile: src/nvidia_rag/rag_server/Dockerfile
    # start the server on port 8081 with 8 workers for improved latency on concurrent requests.
    command: --port 8081 --host 0.0.0.0 --workers 8
    volumes:
      # Mount the prompt.yaml file to the container, path should be absolute
      - ../../src/nvidia_rag/rag_server/prompt.yaml:${PROMPT_CONFIG_FILE:-/prompt.yaml}
    # Common customizations to the pipeline can be controlled using env variables
    environment:
      # Path to example directory relative to root
      EXAMPLE_PATH: './nvidia_rag/rag_server'

      # Absolute path to custom prompt.yaml file
      PROMPT_CONFIG_FILE: ${PROMPT_CONFIG_FILE:-/prompt.yaml}

      ##===MINIO specific configurations which is used to store the multimodal base64 content===
      MINIO_ENDPOINT: "minio:9010"
      MINIO_ACCESSKEY: "minioadmin"
      MINIO_SECRETKEY: "minioadmin"

      ##===Vector DB specific configurations===
      # URL on which vectorstore is hosted
      APP_VECTORSTORE_URL: "http://milvus:19530"
      # Type of vectordb used to store embedding supported type milvus
      APP_VECTORSTORE_NAME: "milvus"
      # Type of index to be used for vectorstore
      APP_VECTORSTORE_INDEXTYPE: ${APP_VECTORSTORE_INDEXTYPE:-"GPU_CAGRA"}
      # Type of vectordb search to be used
      APP_VECTORSTORE_SEARCHTYPE: ${APP_VECTORSTORE_SEARCHTYPE:-"dense"} # Can be dense or hybrid
      # vectorstore collection name to store embeddings
      COLLECTION_NAME: ${COLLECTION_NAME:-multimodal_data}
      APP_RETRIEVER_SCORETHRESHOLD: 0.25
      # Top K from vector DB, which goes as input to reranker model if enabled, else goes to LLM prompt
      VECTOR_DB_TOPK: ${VECTOR_DB_TOPK:-100}

      ##===LLM Model specific configurations===
      APP_LLM_MODELNAME: ${APP_LLM_MODELNAME:-"nvidia/llama-3.3-nemotron-super-49b-v1"}
      # url on which llm model is hosted. If "", Nvidia hosted API is used
      APP_LLM_SERVERURL: ${APP_LLM_SERVERURL-"nim-llm:8000"}

      ##===Query Rewriter Model specific configurations===
      APP_QUERYREWRITER_MODELNAME: ${APP_QUERYREWRITER_MODELNAME:-"meta/llama-3.1-8b-instruct"}
      # url on which query rewriter model is hosted. If "", Nvidia hosted API is used
      APP_QUERYREWRITER_SERVERURL: ${APP_QUERYREWRITER_SERVERURL-"nim-llm-llama-8b:8000"}

      ##===Embedding Model specific configurations===
      # url on which embedding model is hosted. If "", Nvidia hosted API is used
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL-"nemoretriever-embedding-ms:8000"}
      APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}

      ##===Reranking Model specific configurations===
      # url on which ranking model is hosted. If "", Nvidia hosted API is used
      APP_RANKING_SERVERURL: ${APP_RANKING_SERVERURL-"nemoretriever-ranking-ms:8000"}
      APP_RANKING_MODELNAME: ${APP_RANKING_MODELNAME:-"nvidia/llama-3.2-nv-rerankqa-1b-v2"}
      ENABLE_RERANKER: ${ENABLE_RERANKER:-True}

      ##===VLM Model specific configurations===
      ENABLE_VLM_INFERENCE: ${ENABLE_VLM_INFERENCE:-false}
      APP_VLM_SERVERURL: ${APP_VLM_SERVERURL-"http://vlm-ms:8000/v1"}
      APP_VLM_MODELNAME: ${APP_VLM_MODELNAME:-"nvidia/llama-3.1-nemotron-nano-vl-8b-v1"}

      NVIDIA_API_KEY: ${NGC_API_KEY:?"NGC_API_KEY is required"}

      # Number of document chunks to insert in LLM prompt, used only when ENABLE_RERANKER is set to True
      APP_RETRIEVER_TOPK: ${APP_RETRIEVER_TOPK:-10}

      # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
      LOGLEVEL: ${LOGLEVEL:-INFO}

      # enable multi-turn conversation in the rag chain - this controls conversation history usage
      # while doing query rewriting and in LLM prompt
      ENABLE_MULTITURN: ${ENABLE_MULTITURN:-True}

      # enable query rewriting for multiturn conversation in the rag chain.
      # This will improve accuracy of the retrieiver pipeline but increase latency due to an additional LLM call
      ENABLE_QUERYREWRITER: ${ENABLE_QUERYREWRITER:-False}

      # Choose whether to enable citations in the response
      ENABLE_CITATIONS: ${ENABLE_CITATIONS:-True}

      # Choose whether to enable/disable guardrails
      ENABLE_GUARDRAILS: ${ENABLE_GUARDRAILS:-False}

      # NeMo Guardrails URL when ENABLE_GUARDRAILS is true
      NEMO_GUARDRAILS_URL: ${NEMO_GUARDRAILS_URL:-nemo-guardrails-microservice:7331}

      # number of last n chat messages to consider from the provided conversation history
      CONVERSATION_HISTORY: 5

      # Tracing
      APP_TRACING_ENABLED: "False"
      # HTTP endpoint
      APP_TRACING_OTLPHTTPENDPOINT: http://otel-collector:4318/v1/traces
      # GRPC endpoint
      APP_TRACING_OTLPGRPCENDPOINT: grpc://otel-collector:4317

      # Choose whether to enable source metadata in document content during generation
      ENABLE_SOURCE_METADATA: ${ENABLE_SOURCE_METADATA:-true}

      # Whether to filter content within <think></think> tags in model responses
      FILTER_THINK_TOKENS: ${FILTER_THINK_TOKENS:-true}

      # Whether to enable thinking in the rag chain for llama-3.3-nemotron-super-49b model
      ENABLE_NEMOTRON_THINKING: ${ENABLE_NEMOTRON_THINKING:-false}

      # enable reflection (context relevance and response groundedness checking) in the rag chain
      ENABLE_REFLECTION: ${ENABLE_REFLECTION:-false}
      # Maximum number of context relevance loop iterations
      MAX_REFLECTION_LOOP: ${MAX_REFLECTION_LOOP:-3}
      # Minimum relevance score threshold (0-2)
      CONTEXT_RELEVANCE_THRESHOLD: ${CONTEXT_RELEVANCE_THRESHOLD:-1}
      # Minimum groundedness score threshold (0-2)
      RESPONSE_GROUNDEDNESS_THRESHOLD: ${RESPONSE_GROUNDEDNESS_THRESHOLD:-1}
      # reflection llm
      REFLECTION_LLM: ${REFLECTION_LLM:-"nvidia/llama-3.3-nemotron-super-49b-v1"}
      # reflection llm server url. If "", Nvidia hosted API is used
      REFLECTION_LLM_SERVERURL: ${REFLECTION_LLM_SERVERURL-"nim-llm:8000"}

    ports:
      - "8081:8081"
    expose:
      - "8081"
    shm_size: 5gb
    profiles: ["rag"]

  # Sample UI container which interacts with APIs exposed by rag-server container
  rag-playground:
    container_name: rag-playground
    image: nvcr.io/nvstaging/blueprint/rag-playground:${TAG:-latest}
    build:
      # Set context to repo's root directory
      context: ../../frontend
      dockerfile: ./Dockerfile
      args:
        # API URLs for backend services (only URLs needed at build time for Vite)
        VITE_API_CHAT_URL: "http://rag-server:8081/v1"
        VITE_API_VDB_URL: "http://ingestor-server:8082/v1"
    ports:
      - "8090:3000"
    expose:
      - "3000"
    depends_on:
      - rag-server
    environment:
      # Runtime environment variables for Vite
      # Note: Model configuration is managed by frontend settings store
      VITE_API_CHAT_URL: "http://rag-server:8081/v1"
      VITE_API_VDB_URL: "http://ingestor-server:8082/v1"
      NVWB_TRIM_PREFIX: "true"
    profiles: ["rag"]

  # Milvus can be made GPU accelerated by uncommenting the lines as specified below
  milvus:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.5.3-gpu # milvusdb/milvus:v2.5.3 for CPU
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9010
      KNOWHERE_GPU_MEM_POOL_SIZE: 2048;4096
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-./volumes/milvus}:/var/lib/milvus
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
    #   interval: 30s
    #   start_period: 90s
    #   timeout: 20s
    #   retries: 3
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"
    # Comment out this section if CPU based image is used and set below env variables to False
    # export APP_VECTORSTORE_ENABLEGPUSEARCH=False
    # export APP_VECTORSTORE_ENABLEGPUINDEX=False
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              # count: ${INFERENCE_GPU_COUNT:-all}
              device_ids: ['${VECTORSTORE_GPU_DEVICE_ID:-0}']
    profiles: ["vectordb"]

  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.19
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-./volumes/etcd}:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles: ["vectordb"]

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2025-02-28T09-55-16Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9011:9011"
      - "9010:9010"
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-./volumes/minio}:/minio_data
    command: minio server /minio_data --console-address ":9011" --address ":9010"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9010/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    profiles: ["vectordb"]

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    hostname: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ../config/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "9988:9988" # Prometheus metrics exposed by the collector
      - "8889:8889" # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "9411" # Zipkin receiver
      - "4317:4317" # OTLP gRPC receiver
      - "4318:4318" # OTLP/HTTP receiver
      - "55680:55679" # zpages extension
    depends_on:
      - zipkin
    profiles: ["observability"]

  zipkin:
    image: openzipkin/zipkin
    environment:
      JAVA_OPTS: "-Xms4g -Xmx8g -XX:+ExitOnOutOfMemoryError"
    ports:
      - "9411:9411" # Zipkin UI and API
    profiles: ["observability"]

  prometheus:
    image: prom/prometheus:latest
    command:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --storage.tsdb.retention.time=1h
      - --config.file=/etc/prometheus/prometheus-config.yaml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
      - --web.route-prefix=/
      - --enable-feature=exemplar-storage
      - --enable-feature=otlp-write-receiver
    volumes:
      - ../config/prometheus.yaml:/etc/prometheus/prometheus-config.yaml
    ports:
      - "9090:9090"
    profiles: ["observability"]

  grafana:
    container_name: grafana-service
    image: grafana/grafana
    ports:
      - "3000:3000"
    profiles: ["observability"]

  nemo-guardrails-microservice:
    container_name: nemo-guardrails-microservice
    image: nvcr.io/nvidia/nemo-microservices/guardrails:25.06
    ports:
      - "7331:7331"
    volumes:
      - ./nemoguardrails/config-store:/config-store
    environment:
      CONFIG_STORE_PATH: /config-store
      NIM_ENDPOINT_API_KEY: ${NGC_API_KEY}
      NVIDIA_API_KEY: ${NGC_API_KEY}
      NIM_ENDPOINT_URL: ${NIM_ENDPOINT_URL:-http://nim-llm:8000/v1}
      DEFAULT_CONFIG_ID: ${DEFAULT_CONFIG:-nemoguard}
    depends_on:
      content-safety:
        condition: service_healthy
        required: false
      topic-control:
        condition: service_healthy
        required: false
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:7331/v1/health')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    profiles: ["guardrails"]

  content-safety:
    container_name: llama-3.1-nemoguard-8b-content-safety
    image: nvcr.io/nim/nvidia/llama-3.1-nemoguard-8b-content-safety:1.0.0
    environment:
      - NGC_API_KEY=${NGC_API_KEY}
      - NIM_SERVED_MODEL_NAME=llama-3.1-nemoguard-8b-content-safety
      - NIM_CUSTOM_MODEL_NAME=llama-3.1-nemoguard-8b-content-safety
    user: "${USERID}"
    volumes:
      - ${MODEL_DIRECTORY:-/tmp}:/opt/nim/.cache
    ports:
      - "8123:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['${CONTENT_SAFETY_GPU_ID:-7}']
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/v1/health/ready')"]
      interval: 10s
      timeout: 20s
      retries: 100
    profiles: ["guardrails"]

  topic-control:
    container_name: llama-3.1-nemoguard-8b-topic-control
    image: nvcr.io/nim/nvidia/llama-3.1-nemoguard-8b-topic-control:1.0.0
    environment:
      - NGC_API_KEY=${NGC_API_KEY}
      - NIM_SERVED_MODEL_NAME=llama-3.1-nemoguard-8b-topic-control
      - NIM_CUSTOM_MODEL_NAME=llama-3.1-nemoguard-8b-topic-control
    user: "${USERID}"
    volumes:
      - ${MODEL_DIRECTORY:-/tmp}:/opt/nim/.cache
    ports:
      - "8124:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['${TOPIC_CONTROL_GPU_ID:-6}']
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/v1/health/ready')"]
      interval: 10s
      timeout: 20s
      retries: 100
    profiles: ["guardrails"]

volumes:
  ingest:
  vectordb:
  nim_cache:
    external: true

networks:
  default:
    name: nvidia-rag
