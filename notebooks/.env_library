# === Configuration file ===
export APP_CONFIG_FILE=./config.yaml

# === NVIDIA API Key ===
export NVIDIA_API_KEY=${NGC_API_KEY}

# Ingestor server specific configurations
# === Vector DB specific configurations ===
#export APP_VECTORSTORE_URL=http://localhost:19530
#export APP_VECTORSTORE_NAME=milvus
#export APP_VECTORSTORE_INDEXTYPE=GPU_CAGRA
#export APP_VECTORSTORE_SEARCHTYPE=dense
#export APP_VECTORSTORE_CONSISTENCYLEVEL=Strong
#export APP_VECTORSTORE_ENABLEGPUINDEX=True
#export APP_VECTORSTORE_ENABLEGPUSEARCH=True
export COLLECTION_NAME=test_native
export APP_VECTORSTORE_URL=http://localhost:5080
export APP_VECTORSTORE_NAME=pinecone
export APP_VECTORSTORE_SEARCHTYPE=dense

# === MINIO specific configurations ===
export MINIO_ENDPOINT=localhost:9010
export MINIO_ACCESSKEY=minioadmin
export MINIO_SECRETKEY=minioadmin

# === Embedding Model specific configurations ===
export APP_EMBEDDINGS_SERVERURL=localhost:9080
export APP_EMBEDDINGS_MODELNAME=nvidia/llama-3.2-nv-embedqa-1b-v2
export APP_EMBEDDINGS_DIMENSIONS=2048

# === NV-Ingest Connection Configurations ===
export APP_NVINGEST_MESSAGECLIENTHOSTNAME=localhost
export APP_NVINGEST_MESSAGECLIENTPORT=7670

# === NV-Ingest Extract Configurations ===
export APP_NVINGEST_EXTRACTTEXT=True
export APP_NVINGEST_EXTRACTINFOGRAPHICS=False
export APP_NVINGEST_EXTRACTTABLES=True
export APP_NVINGEST_EXTRACTCHARTS=True
export APP_NVINGEST_EXTRACTIMAGES=False
export APP_NVINGEST_PDFEXTRACTMETHOD=None
export APP_NVINGEST_TEXTDEPTH=page

# === NV-Ingest Splitting Configurations ===
export APP_NVINGEST_CHUNKSIZE=512
export APP_NVINGEST_CHUNKOVERLAP=150
export APP_NVINGEST_ENABLEPDFSPLITTER=True

# === NV-Ingest Caption Model configurations ===
export APP_NVINGEST_CAPTIONMODELNAME=nvidia/llama-3.1-nemotron-nano-vl-8b-v1
export APP_NVINGEST_CAPTIONENDPOINTURL=http://localhost:1977/v1/chat/completions

# Choose whether to store the extracted content in the vector store for citation support
export ENABLE_CITATIONS=True

# Log level for server
export LOGLEVEL=INFO

# [Optional] Redis configuration for task status and result storage
export REDIS_HOST=localhost
export REDIS_PORT=6379
export REDIS_DB=0

# Bulk upload to MinIO
export ENABLE_MINIO_BULK_UPLOAD=True

# --- Additional variables from rag-server ---
export EXAMPLE_PATH=./nvidia_rag/rag_server

# === Vector DB additional configs ===
export APP_RETRIEVER_SCORETHRESHOLD=0.25
export VECTOR_DB_TOPK=100

# === LLM Model specific configurations ===
export APP_LLM_MODELNAME="nvidia/llama-3.3-nemotron-super-49b-v1"
export APP_LLM_SERVERURL=localhost:8999

# === Query Rewriter Model specific configurations ===
export APP_QUERYREWRITER_MODELNAME="meta/llama-3.1-8b-instruct"
export APP_QUERYREWRITER_SERVERURL=localhost:8991

# === Reranking Model specific configurations ===
export APP_RANKING_SERVERURL=localhost:1976
export APP_RANKING_MODELNAME="nvidia/llama-3.2-nv-rerankqa-1b-v2"
export ENABLE_RERANKER=True

# === VLM Model specific configurations ===
export ENABLE_VLM_INFERENCE=False
export APP_VLM_SERVERURL=http://localhost:1977/v1
export APP_VLM_MODELNAME="nvidia/llama-3.1-nemotron-nano-vl-8b-v1"

# Number of document chunks to insert in LLM prompt (when reranker enabled)
export APP_RETRIEVER_TOPK=10

# === Conversation and Query Handling ===
export ENABLE_MULTITURN=True
export ENABLE_QUERYREWRITER=False

# === Guardrails ===
export ENABLE_GUARDRAILS=False
export NEMO_GUARDRAILS_URL=localhost:7331

# === Conversation History ===
export CONVERSATION_HISTORY=5

# === Tracing ===
export APP_TRACING_ENABLED=False
export APP_TRACING_OTLPHTTPENDPOINT=http://localhost:4318/v1/traces
export APP_TRACING_OTLPGRPCENDPOINT=grpc://localhost:4317

# === Source Metadata and Filtering ===
export ENABLE_SOURCE_METADATA=true
export FILTER_THINK_TOKENS=true
export ENABLE_NEMOTRON_THINKING=false

# === Reflection (context relevance/groundedness checking) ===
export ENABLE_REFLECTION=false
export MAX_REFLECTION_LOOP=3
export CONTEXT_RELEVANCE_THRESHOLD=1
export RESPONSE_GROUNDEDNESS_THRESHOLD=1
export REFLECTION_LLM="mistralai/mixtral-8x22b-instruct-v0.1"
export REFLECTION_LLM_SERVERURL=localhost:8998

# === Document Summary Model specific configurations ===
export SUMMARY_LLM="nvidia/llama-3.3-nemotron-super-49b-v1"
export SUMMARY_LLM_SERVERURL=localhost:8999
export SUMMARY_LLM_MAX_CHUNK_LENGTH=50000

# === Temporary directory ===
export TEMP_DIR=./tmp-data/

# === Prompt configuration ===
# Change this to the absolute path of the prompt.yaml file you want to use
# export PROMPT_CONFIG_FILE=src/nvidia_rag/rag_server/prompt.yaml